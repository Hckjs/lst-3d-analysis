#+title: Analysis
#+property: header-args :exports code

* TODO Missing things and bugs
- logging waith lstchain scripts
- run pointings
- irf plots

* Main Snakefile
:PROPERTIES:
 :header-args:  :tangle ./workflow/Snakefile :mkdirp yes
:END:

#+begin_src snakemake

# Definitions only, no actual target
include: "definitions.smk"
include: "rules/dl1.smk"
include: "rules/mc.smk"
include: "rules/dl2.smk"
include: "rules/dl3.smk"
include: "rules/dl4.smk"
include: "rules/dl5.smk"

#+end_src

* Defining Paths etc
:PROPERTIES:
 :header-args:  :tangle ./workflow/definitions.smk :mkdirp yes
:END:

From the config files in [[./configs]], the relevant
variables are constructed and defined in [[./workflow/definitions.smk]].
This includes:
- Conda enviroments
- Paths to configs
- Output directories

I kind of duplicate a lot of effort by collecting them here in
some dictionaries and the index into them in the subtasks instead of
just defining all variables directly, but in my mind this setup makes it easier
to reuse the individual stages and reduces mental overhead because the variables
are actually in the same file. Preference I guess.

It is also not really "nice" to have these things defined as `Path`-Objects,
absolute them and then construct `Path`-Objects again, but I had some issues
with how `Snakemake` defines its `cwd` and this fixed it.
Not claiming its the best way to do it, but it should be fine honestly.
That little overhead is really not holding us back here.

#+name: variables
#+begin_src snakemake
import json
import os
from pathlib import Path

# "Main" paths. Everuthing else is relative to these
scripts_dir = Path("scripts")
env_dir = Path("workflow/envs")
config_dir = Path(config.get("config_dir", "../lst-analysis-config"))
main_config_path = (config_dir / "lst_agn.json").absolute()
build = Path("build") / config_dir.name
analyses = [
    x.name for x in config_dir.iterdir() if x.name.startswith("analysis") and x.is_dir()
]

# Using the one used for the dl1 MCs as I dont reprocess anything
# Best might be to reproduce dl1 mc and data?
# For now this is fine as mcpipe uses the default config from lstchain
# (at least in my test production) and lstchain versions dont differ much in the output
# (hopefully). Thats a fat TODO though...
lstchain_config = build / "lstchain_config.json"
MATPLOTLIBRC = os.environ.get("MATPLOTLIBRC", config_dir / "matplotlibrc")

with open(main_config_path, "r") as f:
    config_agn = json.load(f)
PRODUCTION = config_agn["production"]
DECLINATION = config_agn["declination"]

# Set all configs
# .absolute(), because I am paranoid. Thats a TODO kinda
# It seems to be, that snakemake changes the current working directory and then local paths fail
# Per Analysis configs are not in here. They have fixed names in the analysis-* dirs
CONFIGS = {
    "agn": (config_dir / "lst_agn.json").absolute(),
    "data_selection": (config_dir / "data_selection.json").absolute(),
    "irf_tool": (config_dir / "irf_tool_config.json").absolute(),
    "bkg_model": (config_dir / "bkgmodel.yml").absolute(),
}

OUTDIRS = {
    "data_selection": (build / "data_selection").absolute(),
    "mc_nodes": (
        build / "mc_nodes"
    ).absolute(),  # This is not really configurable, its hard coded in the link script
    "mc": (build / "mc").absolute(),
    "models": (build / "models").absolute(),
    "dl1": (build / "dl1").absolute(),
    "dl2": (build / "dl2").absolute(),
    "dl3": (build / "dl3").absolute(),
    "irfs": (build / "irfs").absolute(),
    "dl4": (build / "dl4").absolute(),
    "dl5": (build / "dl5").absolute(),
}

# Set all enviroments
ENVS = {
    "lstchain": config_agn.get("lstchain_enviroment", "lstchain-v0.10.3"),
    "gammapy": (env_dir / "agn-analysis.yml").absolute(),
    "data_selection": (env_dir / "data-selection.yml").absolute(),
    "background": (env_dir / "background.yml").absolute(),
    "dark_matter": (env_dir / "dark_matter.yml").absolute(),
}

SCRIPTS = {
    "data_selection": (scripts_dir / "data_selection").absolute(),
    "mc": (scripts_dir / "mc").absolute(),
    "irfs": (scripts_dir / "irfs").absolute(),
    "dl1": (scripts_dir / "dl1").absolute(),
    "dl2": (scripts_dir / "dl2").absolute(),
    "dl3": (scripts_dir / "dl3").absolute(),
    "dl4": (scripts_dir / "dl4").absolute(),
    "dl5": (scripts_dir / "dl5").absolute(),
    "dm": (scripts_dir / "dm").absolute(),
}


# TODO This is the most critical part as the further evaluation depends on this checkpoint
# Have to make sure this works as expected
def RUN_IDS(wildcards):
    with open(checkpoints.run_ids.get(**wildcards).output.runlist, "r") as f:
        runs = json.load(f)
    return sorted(set(chain(*runs.values())))


def MC_NODES(wildcards):
    exists = Path(checkpoints.link_mc.get(**wildcards).output.dummy).exists()
    mc_nodes = Path(OUTDIRS["mc_nodes"])/ f"{wildcards.particle}"
    nodes = [x.name for x in mc_nodes.glob("*") if x.is_dir()]
    return nodes

def MC_NODES_DL1(wildcards):
    out = Path(OUTDIRS["mc"]) / f"{wildcards.particle}/dl1"
    nodes = MC_NODES(wildcards)
    return [out/f"{node}_{wildcards.train_or_test}.dl1.h5"
            for node in nodes]

def MC_NODES_DL2(wildcards):
    out = Path(OUTDIRS["mc"]) / f"{wildcards.particle}/dl2"
    nodes = MC_NODES(wildcards)
    return [out/f"{node}_{wildcards.train_or_test}.dl2.h5"
            for node in nodes]

# TODO: cuts are not really IRFs, should separate that.
# Add radmax here if 1D
irfs_to_produce = ["aeff", "gh_cut", "edisp", "psf"]  # TODO script missing

def MC_NODES_IRFs(wildcards):
    exists = Path(checkpoints.link_mc.get(**wildcards).output.dummy).exists()
    out = Path(OUTDIRS["irfs"]) / "plots"
    mc_nodes = Path(OUTDIRS["mc_nodes"])/ "GammaDiffuse"
    nodes = [x.name for x in mc_nodes.glob("*") if x.is_dir()]
    return [out/f"{irf}_{node}.pdf" for node in nodes for irf in irfs_to_produce]

models_to_train = [
    Path(OUTDIRS["models"]) / "reg_energy.sav",
    Path(OUTDIRS["models"]) / "cls_gh.sav",
    Path(OUTDIRS["models"]) / "reg_disp_norm.sav",
    Path(OUTDIRS["models"]) / "cls_disp_sign.sav",
]

def DL2_FILES(wildcards):
    ids = RUN_IDS(wildcards)
    out = Path(OUTDIRS["dl2"])
    return [out / f"LST-1.Run{run_id}.dl2.h5" for run_id in ids]


def DL3_FILES(wildcards):
    ids = RUN_IDS(wildcards)
    out = Path(OUTDIRS["dl3"])
    return [out / f"LST-1.Run{run_id}.dl3.fits.gz" for run_id in ids]


def IRF_FILES(wildcards):
    ids = RUN_IDS(wildcards)
    out = Path(OUTDIRS["dl3"])
    return [out / f"irfs_{run_id}.fits.gz" for run_id in ids]


def BKG_FILES(wildcards):
    ids = RUN_IDS(wildcards)
    out = Path(OUTDIRS["dl3"])
    return [out / f"bkg_{run_id}.fits.gz" for run_id in ids]


#+end_src

* DL1
:PROPERTIES:
 :header-args:  :tangle ./workflow/rules/dl1.smk :mkdirp yes
:END:

This stage is arguably the most comlicated one.
On the one hand, I do not even produce the dl1 files, instead using the LSTOSA files,
but on the other hand this is where the magic happens as we go from
"files somewhere on the cluster" to "nicely organized in the build directory".
At a previous point in time, this was referred to as linking and selecting rather than dl1,
but I wanted to have a structure where every stage was more or less one datalevel,
because I disliked the "preselection, selection, selecting mcs" naming,
that followed from the previous structure.

*Note:* If there is a need to calculate DL1 as well, this is pretty straightforward here:
Just link the DL0 instead and have a rule, that creates dl1 from that similar to
[[nameref:dl1_to_dl2_rule][the dl1 to dl2 rule]].
That would then also need to be done for the simulations as well and you probably want to use your own
`lstchain`-config instead of linking the one used for the models like its done right now.


** Define stuff
Important here (besides having the paths defined):
There are some rules, that are really not computationally heavy.
It would be a shame to have the slurm overhead for every step here, so
they are `localrules`.
The linking step creates a dummy file `runs-linked.txt`, that acts as a checkpoint for
the later steps.

#+name: dl1_vars
#+begin_src snakemake

env = ENVS["data_selection"]
config = CONFIGS["data_selection"]
scripts = Path(SCRIPTS["data_selection"])
out = Path(OUTDIRS["data_selection"])
dl1_link_location = Path(OUTDIRS["dl1"])
plots = out / "plots"


run_selection_plots = [
    plots / f"{name}.pdf"
    for name in ["moon-illumination", "cosmics", "cosmics-above", "run-pointings"]
]


rule dl1:
    input:
        out / "runs-linked.txt",
        run_selection_plots,


localrules:
    runlist,
    select_datasets,
    merge_datachecks,
    run_ids,
    data_check,

#+end_src


There is one manual step required before this point:
The runlist has to be downloaded from the lst1 website, which is password-protected.
As I want to have this public, I cannot put the credentials here.
It is just a simple `curl` command, so not a big deal.

*Note:* The analysis will not know of new runs until you redownload the runlist.
That should not matter most of the times, but keep it in mind!


#+begin_src snakemake :header-args: :tangle no
rule runlist:
    output:
        out / "runlist.html",
    shell:
        """
        echo 'Provide the file {output}. The command is:'
        echo 'curl --user <username>:<password> https://lst1.iac.es/datacheck/lstosa/LST_source_catalog.html -o {output}'
        echo 'You might need to create the output directory first.'
        """
#+end_src

** Select relevant runs based on runlist

First of all, we need to select runs observing our source(s) of interest.
This is done purely based on the `runlist.html` without any notion of data quality.
There is a column `Source Name` in there, which should match the source.
A natural expansion here would be to select multiple source names at once if thats useful
for the further analysis steps (-> Background model?)

#+begin_src snakemake

rule select_datasets:
    output:
        out / "runlist.csv",
    input:
        data=out / "runlist.html",
        config=config,
        script=scripts / "select-data.py",
    conda:
        env
    log:
        out=out / "select_datasets.log",
        err=out / "select_datasets.err",
    shell:
        "python {input.script} {input.data} {output} -c {input.config}"

#+end_src

** Data quality checks

The next step is discarding runs, that, for one reason or another, do not
qualify for further analysis.
Luckily, [[https://github.com/cta-observatory/lstosa][LSTOSA]] produces datacheck-files, that we can use here.
A datacheck-file contains runwise statistics of some quantities commonly used to gauge
data quality, for example the rate of cosmic events, which directly translates to
some form of efficiency as the rate of hadrons arriving at earth is considered to be constant.

For ease of use, we merge the runwise datachecks first and make the selection on that
merged object.

#+begin_src snakemake

rule merge_datachecks:
    output:
        output=out / "dl1-datachecks-merged.h5",
    input:
        data=out / "runlist.csv",
        script=scripts / "merge-datachecks.py",
    conda:
        env
    log:
        out / "merge_datacheck.log",
    shell:
        "python {input.script} {input.data} {output.output} --log-file {log}"

#+end_src

That selection is based on one of the config files, where e.g. thresholds for the cosmics rate are set.
As a result, the rule produces:
- A list of selected runs
- Cuts and masked datacheck for plots

#+name: data_check
#+begin_src snakemake

rule data_check:
    output:
        runlist=out / "runlist-checked.csv",
        datachecks=out / "dl1-datachecks-masked.h5",
        config=out / "dl1-selection-cuts-config.json",
    input:
        runlist=out / "runlist.csv",
        datachecks=out / "dl1-datachecks-merged.h5",
        config=config,
        script=scripts / "data-check.py",
    conda:
        env
    log:
        out / "datacheck.log",
    shell:
        "python \
            {input.script} \
            {input.runlist} \
            {input.datachecks} \
            --config {input.config} \
            --output-runlist {output.runlist} \
            --output-datachecks {output.datachecks} \
            --output-config {output.config} \
            --log-file {log}"

#+end_src

** Define runs to be used for the analysis

With the list of runs from [[nameref:data_check]], we can go ahead and
link the runs from the global data-directory to our build-directory.
This simplifies rules massively.
Before doing that, we first convert the runlist.
That is pretty arbitrary and could also be done in a single step.
I do not know exactly why we did it this way, but it works, right?


#+begin_src snakemake

checkpoint run_ids:
    output:
        runlist=out / "runs.json",
    input:
        data=out / "runlist-checked.csv",
        config=config,
        script=scripts / "create-night-run-list.py",
    conda:
        env
    log:
        out / "check_runlist.log",
    shell:
        "python \
        {input.script} \
        {input.data} \
        {output} \
        -c {input.config} \
        --log-file {log}"

#+end_src

Now we arrive at the first checkpoint!
It is important to have this as a checkpoint, because a priori
you do not know which runs you select for the analysis.
It is known a few steps before this to be exact, but since this is the last
(run-)linking related rule, I decided to make this the checkpoint.
From now on out, all run ids are known just by looking into the `<build_dir>/dl1`
folder.

#+name: link_runs_checkpoint
#+begin_src snakemake

checkpoint link_runs:
    output:
        out / "runs-linked.txt",
    input:
        runs=out / "runs.json",
        datacheck=out / "dl1-datachecks-masked.h5",
        script=scripts / "link-runs.py",
    params:
        dl1=dl1_link_location,
    conda:
        env
    log:
        out / "link_runs.log"
    shell:
        "python \
        {input.script} \
        --runs {input.runs} \
        --dl1-link-dir {params.dl1} \
        --log-file {log} \
        --output-path {output}"

#+end_src

** Plots

Plotting the data-selection part is very easy.
Since multiple plots can be constructed from the output of the [[nameref:data_check][datacheck-rule]],
there is just one [[nameref:plot_data_selection][rule]] to handle these and the script-name is constructed from the wildcard
/name/, which is the name of the output plot.
It could also be multiple rules as not all of them need all of the input files,
but this is how we constructed it a while back for the 1D-analysis.

#+name: plot_data_selection
#+begin_src snakemake

rule plot_data_selection:
    output:
        plots / "{name}.pdf",
    input:
        data=out / "dl1-datachecks-masked.h5",
        config=out / "dl1-selection-cuts-config.json",
        script=scripts / "plot-{name}.py",
    conda:
        env
    log:
        plots / "{name}.log",
    shell:
        "python \
        {input.script} \
        {input.data} \
        -c {input.config} \
        -o {output} \
        --log-file {log} "

#+end_src

For the run pointings, a new file containing just these is constructed.
This is actually not a big step and could be done in the plot script aswell,
but I like having the csv file with the pointing directions easily accesible.

TODO This is not working!

#+begin_src snakemake

rule gather_run_pointings:
    output:
        out / "run-pointings.csv",
    input:
        runs=out / "runs.json",
        datacheck=out / "dl1-datachecks-masked.h5",
        script=scripts / "gather-run-pointings.py",
    conda:
        env
    log:
        out / "run_pointings.log",
    shell:
        "python {input.script} \
        --runs {input.runs} \
        --runsummary {input.datacheck} \
        --output {output} \
        --log-file {log} "


rule plot_run_pointings:
    output:
        plots / "run-pointings.pdf",
    input:
        pointings=out / "run-pointings.csv",
        script=scripts / "plot-run-pointings.py",
    conda:
        env
    log:
        plots / "run_pointings.log",
    shell:
        "python {input.script} \
        --input {input.pointings} \
        --output {output} \
        --log-file {log} "
#+end_src

* MC
:PROPERTIES:
 :header-args:  :tangle ./workflow/rules/mc.smk :mkdirp yes
:END:
** Variables

#+begin_src snakemake
lstchain_env = ENVS["lstchain"]
link_env = ENVS["data_selection"]
plot_env = ENVS["gammapy"]
scripts = Path(SCRIPTS["mc"])
mc = Path(OUTDIRS["mc"])
models = mc / "models"

# Need some extra dirs
mc_nodes = Path(OUTDIRS["mc_nodes"])
dl1 = Path(OUTDIRS["dl1"])
models = Path(OUTDIRS["models"])
config = lstchain_config

plots = mc / "plots"

# TODO Configurable
train_size = 0.4



rule mc:
    input:
        link=mc / "mc-linked.txt",
        models=models_to_train,

localrules:
    link_mc,

#+end_src

** Link nodes
#+begin_src snakemake

checkpoint link_mc:
    output:
        dummy=mc / "mc-linked.txt",
        config=config,
    input:
        script=scripts / "link-mc.py",
    params:
        production=PRODUCTION,
        declination=DECLINATION,
        mc_nodes=mc_nodes,
    conda:
        link_env
    log:
        mc / "link_mc.log"
    shell:
        "python \
        {input.script} \
        --prod {params.production} \
        --dec {params.declination} \
        --mc-nodes-link-dir {params.mc_nodes} \
        --model-config-link-path {output.config} \
        --log-file {log} \
        --verbose \
        --output-path {output.dummy}"

#+end_src

** Create train and test files per node

First of all, the individual runs of a single allsky node need to be merged.
After this step there will 2 (train+test) diffuse gamma files per node.

#+begin_src snakemake

rule merge_gamma_mc_per_node:
    output:
        train=mc / "GammaDiffuse/dl1/{node}_train.dl1.h5",
        test=mc / "GammaDiffuse/dl1/{node}_test.dl1.h5",
    input:
        dummy=mc / "mc-linked.txt",
        script=scripts/"merge_mc_nodes.py",
    params:
        train_size=train_size,
        directory=lambda wildcards: mc_nodes / f"GammaDiffuse/{wildcards.node}",
    conda:
        lstchain_env
    log:
        mc / "GammaDiffuse/dl1/merge_gamma_mc_{node}.log",
    shell:
        "python {input.script} \
        --input-dir {params.directory} \
        --train-size {params.train_size} \
        --output-train {output.train} \
        --output-test {output.test} \
        --pattern 'dl1_*.h5' \
        --log-file {log}"

rule merge_proton_mc_per_node:
    output:
        train=mc / "Protons/dl1/{node}_train.dl1.h5",
    input:
        dummy=mc / "mc-linked.txt",
        script=scripts/"merge_mc_nodes.py",
    params:
        train_size=1.0,
        directory=lambda wildcards: mc_nodes / f"Protons/{wildcards.node}",
    conda:
        lstchain_env
    log:
        mc / "Protons/dl1/merge_proton_mc_{node}.log",
    shell:
        "python {input.script} \
        --input-dir {params.directory} \
        --train-size {params.train_size} \
        --output-train {output.train} \
        --pattern 'dl1_*.h5' \
        --log-file {log}"

#+end_src

** Train models

There is only one set of models for the whole trajectory and not one for each node in order
to make better use of the training statistic.

#+begin_src snakemake

rule merge_train_or_test_of_all_nodes:
    output:
        mc / "{particle}/{particle}_{train_or_test}.dl1.h5",
    input:
        nodes=MC_NODES_DL1,
        script=scripts/"merge_mc_nodes.py",
    params:
        directory=lambda wildcards: mc / f"{wildcards.particle}/dl1",
        pattern=lambda wildcards: f"*_{wildcards.train_or_test}.dl1.h5",
        out_type=lambda wildcards: f"output-{wildcards.train_or_test}",
    conda:
        lstchain_env
    log:
        mc / "{particle}/merge_all_{particle}_{train_or_test}.log",
    shell:
        """
        python {input.script} \
        --input-dir {params.directory} \
        --pattern {params.pattern} \
        --{params.out_type} {output} \
        --log-file {log}
        """

#+end_src

For the training it is just the lstchain script.
That requires a lot of resources, because they load all of the data into RAM at once...


#+begin_src snakemake

# TODO Any chance to get logging in here?
rule train_models:
    output:
        models_to_train,
    input:
        gamma=mc / "GammaDiffuse/GammaDiffuse_train.dl1.h5",
        proton=mc / "Protons/Protons_train.dl1.h5",
        config=config,
    resources:
        mem_mb=64000,
        cpus=8,
        partition="long",
        time=1200,
    conda:
        lstchain_env
    log:
        models / "train_models.log",
    shell:
        """
        lstchain_mc_trainpipe \
        --fg {input.gamma} \
        --fp {input.proton} \
        --config {input.config} \
        --output-dir {models}
        """

#+end_src

* DL2
:PROPERTIES:
 :header-args:  :tangle ./workflow/rules/dl2.smk :mkdirp yes
:END:
This stage contains:
- dl1 to dl2 for data
- dl1 to dl2 for test mc
- calculate IRFs (at MC nodes!)
- Plot IRFs

It is not very complicated, because the annoying work of linking and organizing files
has been done before.

** Definition

The target is actually super simple here:
All IRFs and all dl2 files.

These are defined in [[variables]] and include the linking checkpoints.

#+begin_src snakemake
lstchain_env = ENVS["lstchain"]
plot_env = ENVS["gammapy"]
irfs = Path(OUTDIRS["irfs"])
irf_config = CONFIGS["irf_tool"]
irf_scripts = Path(SCRIPTS["irfs"])
dl2 = Path(OUTDIRS["dl2"])
dl2_scripts = Path(SCRIPTS["dl2"])
models = Path(OUTDIRS["models"])
config = lstchain_config
plots = dl2 / "plots"

rule dl2:
    input:
        irfs=MC_NODES_IRFs,
        runs=DL2_FILES,
        irf_plots = [plots / f"{irf}.pdf" for irf in irfs_to_produce]
#+end_src

** Create DL2

Here we just use the lstchain script with the newly created models.
The `{somepath}` wildcard makes it so that this rule works for both the
observed runs and the MC. Organizing files starts to pay off!

#+name: dl1_to_dl2_rule
#+begin_src snakemake
rule dl1_to_dl2:
    output:
        Path("{somepath}/dl2") / "{base}.dl2.h5",
    input:
        data = Path("{somepath}/dl1") / "{base}.dl1.h5",
        config=config,
        models=models_to_train,
    conda:
        lstchain_env
    resources:
        mem_mb=64000,
        cpus=4,
    log:
        "{somepath}/dl2/dl1_to_dl2_{base}.log"
    shell:
        """
        lstchain_dl1_to_dl2  \
            --input-file {input.data}  \
            --output-dir $(dirname {output}) \
            --path-models {models}  \
            --config {input.config}
        """
#+end_src

** IRFs
Calculating IRFs is an easy `lstchain` task.
Note that these mc nodes are now not connected to the data at all (opposed to how it was
done in the old 1D-analysis).
This gives us more flexibility with regard to how that matching should be done and
is more in line with the `lstchain 0.10` way of doing things, where the dl3 step
gets all IRFs every time. That is needed to interpolate the IRFs, but works the same
way when selecting the nearest one.
In older `lstchain`-versions you would have to match by hand and give the script
a single set of IRFs for a single observed run.

Another noteworthy thing here is, that we include the energy dispersion fix
for pyirf. This was discussed in the lst-analysis call from 2023-08-28 and
on the ICRC before that. Basically `pirf` produced wrongly normalized energy dispersion
files, which actually seems to have an impact on higher level analyses.
The script comes directly from the pyirf release (https://github.com/cta-observatory/pyirf/releases/tag/v0.10.0).

#+begin_src snakemake
rule irf:
    output:
        irfs / "irfs_{node}.fits.gz",
    input:
        gammas=mc / "GammaDiffuse/dl2/{node}_test.dl2.h5",
        config=irf_config,
        edisp_script=irf_scripts / "fix_edisp.py",
    conda:
        lstchain_env
    resources:
        mem_mb=8000,
        time=10,
    log:
        irfs / "irfs_{node}.log"
    shell:
        """
        lstchain_create_irf_files \
            -o {output} \
            -g {input.gammas} \
            --config {input.config}

        python {input.edisp_script} {output}
        """
#+end_src

Plotting is a single rule although every IRF is saved individually.
This works by naming the plot scripts in a predictable way of
`plot_{irf}.py` and have them all behave the same way w.r.t. cli arguments.
The wildcard constraint makes it possible to have two wildcards in the output filename.
Otherwise `irf` could match on e.g. `aeff_node_xzy_...` and `base` would only match
on the last part. Maybe one could also change the behaviour of the regex matching there,
but I think this solution is pretty nice.

#+begin_src snakemake
rule plot_irf:
    output:
        "{somepath}/plots/{irf}_{base}.pdf",
    input:
        data="{somepath}/irfs_{base}.fits.gz",
        script=irf_scripts / "plot_irf_{irf}.py",
        rc=MATPLOTLIBRC,
    conda:
        plot_env
    resources:
        mem_mb=1000,
        time=20,
    wildcard_constraints:
        irf="|".join(irfs_to_produce)
    log:
        "{somepath}/plots/{irf}_{base}.log"
    shell:
        "MATPLOTLIBRC={input.rc} \
        python {input.script} \
        -i {input.data} \
        -o {output} \
        --log-file {log}"

#+end_src

* DL3
:PROPERTIES:
 :header-args:  :tangle ./workflow/rules/dl3.smk :mkdirp yes
:END:
This stage is rather complex.
On the one hand, creating dl3 files from dl2 and IRFs is not difficult at all with the
existing `lstchain` infrastructure.
There are a lot of plots created here and skymaps are calculated,
but that is not complicated, just a lot of rules.
On the other hand, we need a background for the dl4 datasets.
That is not a default `lstchain` task and needs to be done by hand.
There is a `pybkgmodel` based script, that produces a background for every observation
and then a small helper script to add the background "IRF" to the hdu-index.

** Definitions
Nothing crazy here.
Note, that the `irfs` path usually relates to the grid IRFs and the
"final" IRFs, that are linked in the dl3 files, lie in the dl3 path.

Need to define the runwise plots extra here, because exapdn does not seem to work with
the RUN_IDS function (remember its not just a variable due to the checkpoint thing)

#+begin_src snakemake
lstchain_env = ENVS["lstchain"]
bkg_env = ENVS["background"]
gammapy_env = ENVS["gammapy"]

dl2 = Path(OUTDIRS["dl2"])
dl3 = Path(OUTDIRS["dl3"])
plots = dl3 / "plots"
irfs = Path(OUTDIRS["irfs"])

scripts = Path(SCRIPTS["dl3"])

irf_config = CONFIGS["irf_tool"]
bkg_config = CONFIGS["bkg_model"]
data_selection_config = CONFIGS["data_selection"]

dl3_plot_types = ["theta2", "skymap", "counts_after_cuts", "bkg"]

irf_plots = [plots / f"{irf}.pdf" for irf in irfs_to_produce]
# bkg plots as well!

def DL3_PLOTS(wildcards):
    ids = RUN_IDS(wildcards)
    per_run = [dl3 / f"plots/{p}/{p}_{run}.pdf" for p in dl3_plot_types for run in ids]
    return per_run + [dl3/f"plots/{p}/{p}_stacked.pdf" for p in dl3_plot_types[:-1]]

rule dl3:
    input:
        index=dl3 / "hdu-index.fits.gz",
        bkg=dl3/"bkg-exists",
        runwise_plots=DL3_PLOTS,
        irf_plots=irf_plots,

#+end_src

** Create DL3 Data
*** Individual runs
Thats just lstchain here.
Using the (new at the time) interface of 0.10.x, this is incompatible with
previous versions. I could make an attempt to support that as well, but I do not
see why I would use previous versions again, so might as well force it.

#+begin_src snakemake

rule dl2_to_dl3:
    output:
        run=dl3 / "LST-1.Run{run_id}.dl3.fits.gz",
    input:
        data=dl2 / "LST-1.Run{run_id}.dl2.h5",
        irfs=MC_NODES_IRFs,
        config=irf_config,
    params:
        irf_pattern='irfs_*.fits.gz',
        out=dl3,
        in_irfs=irfs,
    conda:
        lstchain_env
    resources:
        mem_mb=12000,
        time=30,
    log:
        dl3 / "create_dl3_{run_id}.log"
    shell:
        """
        lstchain_create_dl3_file  \
            --input-dl2 {input.data}  \
            --output-dl3-path {params.out}  \
            --input-irf-path {params.in_irfs}  \
            --irf-file-pattern {params.irf_pattern} \
            --config {input.config} \
            --gzip \
            --use-nearest-irf-node \
            --overwrite \
            --log-file {log}
        """
#+end_src
*** Background
After some back and forth with pybkgmodel, I decided to write my own script.
Its content is subject to change, but some design goals will probably stay the same, so
the workflow will not change much (hopefully):
1) Runwise background. There can be identical models, but every run gets a background model
   file with the run_id included. Not changing this around when eventually producing
   stacked models makes the dl3 construction simpler.
2) One call to the script creates all backgrounds.
   This is opposed to the one to one matching used throughout the workflow,
   but it makes me more flexible w.r.t script development.
   All runs are available at once meaning I can match and group them however I want.
   I could still read all runs every time as it is not THAT expensive (main part being
   the transformations of the event list and not I/O of the datastore and metadata),
   but this way you could e.g. create N background models for N zenith bins and
   use them for all runs in the bin thus effectively constructing less models.

There is a basic plot script here as well for visual inspection of things like
radial symmetry and empty bins.

#+begin_src snakemake

rule calc_count_maps:
    output:
        dl3 / "bkg_cached_maps.pkl"
    input:
        runs=DL3_FILES,
        config=bkg_config,
        script=scripts/"precompute_background_maps.py",
        bkg_exclusion_regions=config_dir/"bkg_exclusion",
    params:
        obs_dir=dl3,
        bkg_dir=dl3,
    conda:
        bkg_env
    resources:
        partition="long",
        time=360,
    log:
        dl3 / "calc_count_maps.log",
    shell:
        """python {input.script} \
        --input-runs {input.runs} \
        --exclusion {input.bkg_exclusion_regions} \
        --output {output} \
        --config {input.config} \
        --log-file {log} \
        --overwrite
        """

rule calc_background:
    output:
        dummy=dl3/"bkg-exists",
    input:
        runs=DL3_FILES,
        config=bkg_config,
        script=scripts/"calc_background.py",
        cached_maps=dl3/"bkg_cached_maps.pkl",
        bkg_exclusion_regions=config_dir/"bkg_exclusion",
    params:
        obs_dir=dl3,
        bkg_dir=dl3,
    conda:
        bkg_env
    resources:
        partition="short",
    log:
        dl3 / "calc_bkg.log",
    shell:
        """python {input.script} \
        --input-runs {input.runs} \
        --output-dir {params.bkg_dir} \
        --exclusion {input.bkg_exclusion_regions} \
        --dummy-output {output.dummy} \
        --cached-maps {input.cached_maps} \
        --config {input.config} \
        --log-file {log} \
        --overwrite
        """
#+end_src

*** Index
The actual work consists of calling the lstchain script and then adding
new rows to the table, that link to the background files.
The script is very basic and assumes, that you can sort the background files and get the
same order as when sorting the observations.
That is true as long as the name of the background file contains the run_id
and there are no shenanigans with the leading 0.


#+begin_src snakemake

rule dl3_hdu_index:
    output:
        dl3 / "hdu-index.fits.gz",
    input:
        runs=DL3_FILES,
        link_script=scripts / "link_bkg.py",
        dummy=dl3/"bkg-exists",
    params:
        outdir=dl3,
        bkg=BKG_FILES,
    conda:
        lstchain_env
    log:
        dl3 / "hdu_index.log",
    shell:
        """
        lstchain_create_dl3_index_files  \
            --input-dl3-dir {params.outdir}  \
            --output-index-path {params.outdir}  \
            --file-pattern '*.dl3.fits.gz'  \
            --overwrite \
            --log-file {log}

        python {input.link_script} \
        --hdu-index-path {output} \
        --bkg-files {params.bkg} \
        """

#+end_src


** Plot dl3 stuff
*** Theta 2
#+begin_src snakemake
rule calc_theta2_per_obs:
    output:
        dl3 / "theta2/{run_id}.fits.gz",
    input:
        data=dl3 / "LST-1.Run{run_id}.dl3.fits.gz",
        script=scripts/"calc_theta2_per_obs.py",
        config=data_selection_config, # this seems unnecessary
        index=dl3 / "hdu-index.fits.gz",
    wildcard_constraints:
        run_id="\d+",  # dont match on "stacked".
    resources:
        mem_mb=16000,
    conda:
        gammapy_env
    log:
        dl3 / "theta2/calc_{run_id}.log",
    shell:
        "python {input.script} -i {dl3} -o {output} --obs-id {wildcards.run_id} --config {input.config} --log-file {log}"

def dl3_all_theta_tables(wildcards):
    ids = RUN_IDS(wildcards)
    return [dl3 / f"theta2/{run}.fits.gz" for run in ids]

rule stack_theta2:
    output:
        dl3 / "theta2/stacked.fits.gz",
    input:
        runs=dl3_all_theta_tables,
        script=scripts/"stack_theta2.py",
    conda:
        gammapy_env
    log:
        dl3 / "theta2/theta2_stacked.log",
    shell:
        "python {input.script} -o {output} --input-files {input.runs} --log-file {log}"


rule plot_theta:
    output:
        plots / "theta2/theta2_{run_id}.pdf",
    input:
        data=dl3 / "theta2/{run_id}.fits.gz",
        script=scripts/"plot_theta2.py",
        rc=MATPLOTLIBRC,
    conda:
        gammapy_env
    log:
        plots / "theta2/plot_{run_id}.log",
    shell:
        "MATPLOTLIBRC={input.rc} python {input.script} -i {input.data} -o {output} --log-file {log}"

#+end_src
*** Background

super suboptimal due to the bkg thing
#+begin_src snakemake

rule plot_background:
    output:
        plots / "bkg/bkg_{run_id}.pdf"
    input:
        data=dl3 / "bkg-exists",
        script=scripts / "plot_bkg.py",
        rc=MATPLOTLIBRC,
    params:
        data=lambda wildcards: dl3 / f"bkg_{wildcards.run_id}.fits.gz",
    conda:
        gammapy_env
    log:
        dl3 / "plots/bkg/bkg_{run_id}.log",
    shell:
        "MATPLOTLIBRC={input.rc} python {input.script} -i {params.data} -o {output}"

#+end_src
*** Skymaps

#+begin_src snakemake
rule calc_skymap:
    output:
        dl3 / "skymap/{run_id}.fits.gz",
    input:
        data=dl3 / "LST-1.Run{run_id}.dl3.fits.gz",
        script=scripts/"calc_skymap_gammas.py",
        config=irf_config,
        index=dl3 / "hdu-index.fits.gz",
    wildcard_constraints:
        run_id="\d+",  # dont match on "stacked".
    resources:
        # mem_mb=16000,
        time=5,
    conda:
        gammapy_env
    log:
        dl3 / "skymap/calc_{run_id}.log",
    shell:
        "python {input.script} -i {dl3} -o {output} --obs-id {wildcards.run_id} --config {input.config} --log-file {log}"


def dl3_all_skymaps(wildcards):
    ids = RUN_IDS(wildcards)
    return [dl3 / f"skymap/{run}.fits.gz" for run in ids]


rule stack_skymaps:
    output:
        dl3 / "skymap/stacked.fits.gz",
    input:
        data=dl3_all_skymaps,
        script=scripts/"stack_skymap.py",
    conda:
        gammapy_env
    log:
        dl3 / "skymap/stack.log",
    shell:
        "python {input.script} -i {input.data} -o {output} --log-file {log}"


rule plot_skymap:
    output:
        plots/ "skymap/skymap_{run_id}.pdf",
    input:
        data=dl3 / "skymap/{run_id}.fits.gz",
        script=scripts/"plot_skymap.py",
        rc=MATPLOTLIBRC,
    conda:
        gammapy_env
    resources:
        time=5,
    log:
        plots / "skymap/plot_{run_id}.log",
    shell:
        "MATPLOTLIBRC={input.rc} python {input.script} -i {input.data} -o {output} --log-file {log}"


#+end_src

*** Cuts dl3 (kinda irf)
#+begin_src snakemake
rule cuts_dl2_dl3:
    output:
        dl3 / "counts_after_cuts/{run_id}.h5",
    input:
        dl2=dl2/ "LST-1.Run{run_id}.dl2.h5",
        irf=dl3 / "LST-1.Run{run_id}.dl3.fits.gz",
        config=irf_config,
        script=scripts/"calc_counts_after_cuts.py",
    wildcard_constraints:
        run_id="\d+",  # dont match on "stacked".
    resources:
        mem_mb="64G",
        time=10,
    conda:
       lstchain_env
    log:
        dl3 / "counts_after_cuts/calc_{run_id}.log",
    shell:
        "python {input.script} --input-dl2 {input.dl2} --input-irf {input.irf} -c {input.config} -o {output} --log-file {log}"
#+end_src

#+begin_src snakemake

def dl3_all_counts(wildcards):
    ids = RUN_IDS(wildcards)
    return [dl3 / f"counts_after_cuts/{run}.h5" for run in ids]

rule stack_cuts_dl2_dl3:
    output:
        dl3/ "counts_after_cuts/stacked.h5",
    input:
        data=dl3_all_counts,
        script=scripts/"stack_counts_after_cuts.py",
        rc=MATPLOTLIBRC,
    conda:
        lstchain_env
    log:
        dl3 / "counts_after_cuts/stack.log", # TODO use this
    shell:
        "MATPLOTLIBRC={input.rc} python {input.script} -i {input.data} -o {output}"

#+end_src

#+begin_src snakemake
rule plot_cuts_dl2_dl3:
    output:
        plots/ "counts_after_cuts/counts_after_cuts_{run_id}.pdf",
    input:
        data=dl3/ "counts_after_cuts/{run_id}.h5",
        script=scripts/"plot_counts_after_cuts.py",
        rc=MATPLOTLIBRC,
    conda:
        lstchain_env
    log:
        plots / "counts_after_cuts/plot_counts_{run_id}.log",
    shell:
        "MATPLOTLIBRC={input.rc} python {input.script} -i {input.data} -o {output} --log-file {log}"
#+end_src

* DL4
:PROPERTIES:
 :header-args:  :tangle ./workflow/rules/dl4.smk :mkdirp yes
:END:
** Definitions

#+begin_src snakemake
gammapy_env = ENVS["gammapy"]
dl3 = Path(OUTDIRS["dl3"])
dl4 = Path(OUTDIRS["dl4"])
plots = dl4 / "plots"
scripts = Path(SCRIPTS["dl4"])

dl4_plot_types = ["dataset_peek"]#, "dl4_diagnostics"]
rule dl4:
    input:
        [plots / f"{analysis}/{plot}.pdf" for analysis in analyses for plot in dl4_plot_types]

#+end_src

** Create MapDataset

#+begin_src snakemake

rule create_fov_bkg_exclusion:
    output:
        dl4 / "{analysis}/bkg_exclusion.fits.gz"
    input:
        region=config_dir / "bkg_exclusion",
        script=scripts/"create_fits_exclusion.py",
        config=config_dir / "{analysis}/analysis.yaml",
    conda:
        gammapy_env
    log:
        dl4 / "{analysis}/create_exclusion.log",
    shell:
        "python {input.script}  -i {input.region} -o {output} --log-file {log} -c {input.config}"
#+end_src

#+begin_src snakemake
rule create_dataset:
    output:
        dl4/ "{analysis}/datasets.fits.gz",
    input:
        data=dl3 / "hdu-index.fits.gz",
        config=config_dir / "{analysis}/analysis.yaml",
        script=scripts/"write_datasets_3d.py",
        bkg_exclusion_regions=dl4 / "{analysis}/bkg_exclusion.fits.gz"
    conda:
        gammapy_env
    log:
        dl4/ "{analysis}/datasets.log",
    shell:
        "python {input.script} -c {input.config}  -o {output} --log-file {log}"
#+end_src

** Plot DL4 statistics

#+begin_src snakemake
rule calc_dl4_diagnostics:
    output:
        dl4/ "{analysis}/dl4_diagnostics.fits.gz",
    input:
        data=dl4/"{analysis}/datasets.fits.gz",
        config=config_dir / "{analysis}/analysis.yaml",
        script=scripts/"calc_dl4_diagnostics.py",
    resources:
        mem_mb=16000,
    conda:
        gammapy_env
    log:
        dl4/ "{analysis}/dl4_diagnostics.log",
    shell:
        "python {input.script} -c {input.config} -o {output} --dataset-path {input.data} --log-file {log}"


rule peek_datasets:
    output:
        plots/"{analysis}/dataset_peek.pdf",
    input:
        data=dl4/ "{analysis}/datasets.fits.gz",
        script=scripts/"plot_dataset_peek.py",
        config=config_dir / "{analysis}/analysis.yaml",
        rc=MATPLOTLIBRC,
    conda:
        gammapy_env
    log:
        plots/ "{analysis}/dataset_peek.log",
    shell:
        "MATPLOTLIBRC={input.rc} python {input.script} -c {input.config} -o {output} --dataset-path {input.data} --log-file {log}"


rule plot_dl4_dianotics:
    output:
        plots/ "{analysis}/dl4_diagnostics.pdf",
    input:
        data=dl4/"{analysis}/dl4_diagnostics.fits.gz",
        script=scripts/"plot_dl4_diagnostics.py",
        rc=os.environ.get("MATPLOTLIBRC", config_dir / "matplotlibrc"),
    conda:
        gammapy_env
    log:
        plots/ "{analysis}/dl4_diagnostics.log",
    shell:
        "MATPLOTLIBRC={input.rc} python {input.script} -i {input.data} -o {output} --log-file {log}"

#+end_src

* TODO DL5
:PROPERTIES:
 :header-args:  :tangle ./workflow/rules/dl5.smk :mkdirp yes
:END:

Because most of these steps are a simple calc something -> plot results, I have not put the
plot rules separate here, but instead organised it by the values to fit/compute.

** Define stuff

#+begin_src snakemake
gammapy_env = ENVS["gammapy"]
dl4 = Path(OUTDIRS["dl4"])
dl5 = Path(OUTDIRS["dl5"])
plots = dl5 / "plots"
scripts = Path(SCRIPTS["dl5"])

dl5_plot_types = ["significance_map", "2d_flux_profile"]#, flux_points, light_curve]
rule dl5:
    input:
        [plots / f"{analysis}/{plot}.pdf" for analysis in analyses for plot in dl5_plot_types]

#+end_src
** Significance map
#+begin_src snakemake

rule calc_significance_map:
    output:
        dl5 / "{analysis}/significance_map.fits.gz",
    input:
        data=dl4 / "{analysis}/datasets.fits.gz",
        script=scripts/"calc_significance_map.py",
    conda:
        gammapy_env
    log:
        dl5/ "{analysis}/calc_significance_map.log",
    shell:
        """
        python {input.script} \
        --dataset-path {input.data} \
        --output {output} \
        --log-file {log}
        """
#+end_src

#+begin_src snakemake

rule plot_significance_map:
    output:
        plots / "{analysis}/significance_map.pdf",
    input:
        lima_map=dl5 / "{analysis}/significance_map.fits.gz",
        script=scripts/"plot_significance_map.py",
        rc=MATPLOTLIBRC,
    conda:
        gammapy_env
    log:
        dl5 / "{analysis}/plot_significance_map.log",
    shell:
        """
        MATPLOTLIBRC={input.rc} \
        python {input.script} \
        --flux-maps {input.lima_map} \
        --output {output} \
        --log-file {log}
        """

#+end_src
#+begin_src snakemake

rule plot_significance_distribution:
    output:
        plots / "{analysis}/significance_distribution.pdf",
    input:
        lima_map=dl5 / "{analysis}/significance_map.fits.gz",
        script=scripts/"plot_significance_distribution.py",
        rc=MATPLOTLIBRC,
        exclusion_mask= dl4/ "{analysis}/exclusion.fits.gz",
    conda:
        gammapy_env
    log:
        dl5 / "{analysis}/plot_significance_distribution.log",
    shell:
        """
        MATPLOTLIBRC={input.rc} \
        python {input.script} \
        --input-maps {input.lima_map} \
        --exclusion-mask {input.exclusion_mask} \
        --output {output} \
        --log-file {log}
        """

#+end_src

** 2D Flux profile
https://docs.gammapy.org/1.1/tutorials/analysis-3d/flux_profiles.html

#+begin_src snakemake
rule calc_2d_flux_profile:
    output:
        dl5 / "{analysis}/2d_flux_profile.fits.gz",
    input:
        data=dl4 / "{analysis}/datasets.fits.gz",
        script=scripts/"calc_2d_flux_profile.py",
    conda:
        gammapy_env
    log:
        dl5/ "{analysis}/calc_2d_flux_profile.log",
    shell:
        """
        python {input.script} \
        --dataset-path {input.data} \
        --output {output} \
        --log-file {log}
        """
#+end_src


#+begin_src snakemake

rule plot_2d_flux_profile:
    output:
        plots / "{analysis}/2d_flux_profile.pdf",
    input:
        flux_points=dl5 / "{analysis}/2d_flux_profile.fits.gz",
        script=scripts/"plot_2d_flux_profile.py",
        rc=MATPLOTLIBRC,
    conda:
        gammapy_env
    log:
        dl5 / "{analysis}/plot_2d_flux_profile.log",
    shell:
        """
        MATPLOTLIBRC={input.rc} \
        python {input.script} \
        --flux-points {input.flux_points} \
        --output {output} \
        --log-file {log}
        """

#+end_src
** Fit Skymodel(s)
:PROPERTIES:
 :header-args:  :tangle no
:END:
https://docs.gammapy.org/1.1/tutorials/analysis-3d/analysis_3d.html
- with 3d residuals map
- model für die ghostbuster?

#+begin_src snakemake

rule model_best_fit:
    input:
        config=config_dir / "{analysis}/analysis.yaml",
        dataset=build_dir / "dl4/{analysis}/datasets.fits.gz",
        model=config_dir / "{analysis}/models.yaml",
        script="scripts/fit-model.py",
    output:
        build_dir / "dl4/{analysis}/model-best-fit.yaml",
    conda:
        gammapy_env
    shell:
        """
        python {input.script} \
            -c {input.config} \
            --dataset-path {input.dataset} \
            --model-config {input.model} \
            -o {output} \
        """
#+end_src
#+begin_src snakemake

# Fit flux etc.
rule calc_flux_points:
    input:
        data=build_dir / "dl4/{analysis}/datasets.fits.gz",
        config=config_dir / "{analysis}/analysis.yaml",
        model=build_dir / "dl4/{analysis}/model-best-fit.yaml",
        script="scripts/calc_flux_points.py",
    output:
        build_dir / "dl4/{analysis}/flux_points.fits.gz",
    conda:
        gammapy_env
    shell:
        """
        python {input.script} \
            -c {input.config} \
            --dataset-path {input.data} \
            --best-model-path {input.model} \
            -o {output}
        """

#+end_src
#+begin_src snakemake

rule plot_flux_points:
    input:
        data=build_dir / "dl4/{analysis}/flux_points.fits.gz",
        model=build_dir / "dl4/{analysis}/model-best-fit.yaml",
        script="scripts/plot_flux_points.py",
    output:
        build_dir / "plots/{analysis}/flux_points.pdf",
    conda:
        gammapy_env
    shell:
        """
        python {input.script} \
            -i {input.data} \
            --best-model-path {input.model} \
            -o {output}
        """

#+end_src
#+begin_src snakemake

rule calc_light_curve:
    input:
        model=build_dir / "dl4/{analysis}/model-best-fit.yaml",
        config=config_dir / "{analysis}/analysis.yaml",
        dataset=build_dir / "dl4/{analysis}/datasets.fits.gz",
        script="scripts/calc_light_curve.py",
    output:
        build_dir / "dl4/{analysis}/light_curve.fits.gz",
    conda:
        gammapy_env
    shell:
        """
        python {input.script} \
            -c {input.config} \
            --dataset-path {input.dataset} \
            --best-model-path {input.model} \
            -o {output} \
        """

#+end_src


* TODO DM
:PROPERTIES:
 :header-args:  :tangle ./workflow/rules/dm.smk :mkdirp yes
:END:

ist schon auch irgendwie dl5, aber extra, weil nicht standard gammapy stuff
#+begin_src snakemake
#TODO
#+end_src
