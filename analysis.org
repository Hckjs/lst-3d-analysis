#+title: Analysis
#+property: header-args :exports code

* Main Snakefile
:PROPERTIES:
 :header-args:  :tangle ./workflow/Snakefile :mkdirp yes
:END:

#+begin_src snakemake

# Definitions only, no actual target
include: "definitions.smk"
include: "rules/dl1.smk"
include: "rules/mc.smk"
include: "rules/dl2.smk"

# include: "rules/dl3.smk"
# include: "rules/dl4.smk"
# include: "rules/dl5.smk"

#+end_src

* Defining Paths etc
:PROPERTIES:
 :header-args:  :tangle ./workflow/definitions.smk :mkdirp yes
:END:

From the config files in [[./configs]], the relevant
variables are constructed and defined in [[./workflow/definitions.smk]].
This includes:
- Conda enviroments
- Paths to configs
- Output directories

I kind of duplicate a lot of effort by collecting them here in
some dictionaries and the index into them in the subtasks instead of
just defining all variables directly, but in my mind this setup makes it easier
to reuse the individual stages and reduces mental overhead because the variables
are actually in the same file. Preference I guess.

It is also not really "nice" to have these things defined as `Path`-Objects,
absolute them and then construct `Path`-Objects again, but I had some issues
with how `Snakemake` defines its `cwd` and this fixed it.
Not claiming its the best way to do it, but it should be fine honestly.
That little overhead is really not holding us back here.

#+name: variables
#+begin_src snakemake
import json
import os
from pathlib import Path

# "Main" paths. Everuthing else is relative to these
scripts_dir = Path("scripts")
env_dir = Path("workflow/envs")
config_dir = Path(config.get("config_dir", "../lst-analysis-config"))
main_config_path = (config_dir / "lst_agn.json").absolute()
build = Path("build") / config_dir.name

# Using the one used for the dl1 MCs as I dont reprocess anything
# Best might be to reproduce dl1 mc and data?
# For now this is fine as mcpipe uses the default config from lstchain
# (at least in my test production) and lstchain versions dont differ much in the output
# (hopefully). Thats a fat TODO though...
lstchain_config = build / "lstchain_config.json"
MATPLOTLIBRC = os.environ.get("MATPLOTLIBRC", config_dir / "matplotlibrc")

with open(main_config_path, "r") as f:
    config_agn = json.load(f)
PRODUCTION = config_agn["production"]
DECLINATION = config_agn["declination"]

# Set all configs
# .absolute(), because I am paranoid. Thats a TODO kinda
# It seems to be, that snakemake changes the current working directory and then local paths fail
# Per Analysis configs are not in here. They have fixed names in the analysis-* dirs
CONFIGS = {
    "agn": (config_dir / "lst_agn.json").absolute(),
    "data_selection": (config_dir / "data_selection.json").absolute(),
    "irf_tool": (config_dir / "irf_tool_config.json").absolute(),
    "bkg_model": (config_dir / "bkgmodel.yml").absolute(),
}

OUTDIRS = {
    "data_selection": (build / "data_selection").absolute(),
    "mc_nodes": (
        build / "mc_nodes"
    ).absolute(),  # This is not really configurable, its hard coded in the link script
    "mc": (build / "mc").absolute(),
    "models": (build / "models").absolute(),
    "dl1": (build / "dl1").absolute(),
    "dl2": (build / "dl2").absolute(),
    "dl3": (build / "dl3").absolute(),
    "irfs": (build / "irfs").absolute(),
    "dl4": (build / "dl4").absolute(),
    "dl5": (build / "dl5").absolute(),
}

# Set all enviroments
ENVS = {
    "lstchain": config_agn.get("lstchain_enviroment", "lstchain-v0.10.3"),
    "gammapy": (env_dir / "agn_analysis.yml").absolute(),
    "data_selection": (env_dir / "data-selection.yml").absolute(),
    "background": (env_dir / "background.yml").absolute(),
    "dark_matter": (env_dir / "dark_matter.yml").absolute(),
}

SCRIPTS = {
    "data_selection": (scripts_dir / "data_selection").absolute(),
    "mc": (scripts_dir / "mc").absolute(),
    "dl1": (scripts_dir / "dl1").absolute(),
    "dl2": (scripts_dir / "dl2").absolute(),
    "irfs": (scripts_dir / "irfs").absolute(),
    "dl3": (scripts_dir / "dl3").absolute(),
    "dl4": (scripts_dir / "dl4").absolute(),
    "fit": (scripts_dir / "fit").absolute(),
    "dm": (scripts_dir / "dm").absolute(),
}


# TODO This is the most critical part as the further evaluation depends on this checkpoint
# Have to make sure this works as expected
def RUN_IDS(wildcards):
    with open(checkpoints.run_ids.get(**wildcards).output, "r") as f:
        runs = json.load(f)
    return sorted(set(chain(*runs.values())))


def MC_NODES(wildcards):
    exists = Path(checkpoints.link_mc.get(**wildcards).output.dummy).exists()
    mc_nodes = Path(OUTDIRS["mc_nodes"])/ f"{wildcards.particle}"
    nodes = [x.name for x in mc_nodes.glob("*") if x.is_dir()]
    return nodes

def MC_NODES_DL1(wildcards):
    out = Path(OUTDIRS["mc"]) / f"{wildcards.particle}/dl1"
    nodes = MC_NODES(wildcards)
    return [out/f"{node}_{wildcards.train_or_test}.dl1.h5"
            for node in nodes]

def MC_NODES_DL2(wildcards):
    out = Path(OUTDIRS["mc"]) / f"{wildcards.particle}/dl2"
    nodes = MC_NODES(wildcards)
    return [out/f"{node}_{wildcards.train_or_test}.dl2.h5"
            for node in nodes]

def MC_NODES_IRFs(wildcards):
    exists = Path(checkpoints.link_mc.get(**wildcards).output.dummy).exists()
    out = Path(OUTDIRS["irfs"])
    mc_nodes = Path(OUTDIRS["mc_nodes"])/ "GammaDiffuse"
    nodes = [x.name for x in mc_nodes.glob("*") if x.is_dir()]
    return [out/f"irfs_{node}_.fits.gz" for node in nodes]


def DL2_FILES(wildcards):
    ids = RUN_IDS(wildcards)
    print(ids)
    out = Path(OUTDIRS["dl2"])
    return [out / "LST-1.Run{run_id}.dl2.h5" for run_id in ids]


models_to_train = [
    Path(OUTDIRS["models"]) / "reg_energy.sav",
    Path(OUTDIRS["models"]) / "cls_gh.sav",
    Path(OUTDIRS["models"]) / "reg_disp_norm.sav",
    Path(OUTDIRS["models"]) / "cls_disp_sign.sav",
]




# TODO: cuts are not really IRFs, should separate that.
# Add radmax here if 1D
irfs_to_produce = ["aeff", "gh_cut", "edisp", "psf"]  # TODO script missing

#+end_src

* DL1
:PROPERTIES:
 :header-args:  :tangle ./workflow/rules/dl1.smk :mkdirp yes
:END:

This stage is arguably the most comlicated one.
On the one hand, I do not even produce the dl1 files, instead using the LSTOSA files,
but on the other hand this is where the magic happens as we go from
"files somewhere on the cluster" to "nicely organized in the build directory".
At a previous point in time, this was referred to as linking and selecting rather than dl1,
but I wanted to have a structure where every stage was more or less one datalevel,
because I disliked the "preselection, selection, selecting mcs" naming,
that followed from the previous structure.

*Note:* If there is a need to calculate DL1 as well, this is pretty straightforward here:
Just link the DL0 instead and have a rule, that creates dl1 from that similar to
[[nameref:dl1_to_dl2_rule][the dl1 to dl2 rule]].
That would then also need to be done for the simulations as well and you probably want to use your own
`lstchain`-config instead of linking the one used for the models like its done right now.


** Define stuff
Important here (besides having the paths defined):
There are some rules, that are really not computationally heavy.
It would be a shame to have the slurm overhead for every step here, so
they are `localrules`.
The linking step creates a dummy file `runs-linked.txt`, that acts as a checkpoint for
the later steps.

#+name: dl1_vars
#+begin_src snakemake

env = ENVS["data_selection"]
config = CONFIGS["data_selection"]
scripts = Path(SCRIPTS["data_selection"])
out = Path(OUTDIRS["data_selection"])
dl1_link_location = Path(OUTDIRS["dl1"])
plots = out / "plots"


run_selection_plots = [
    plots / f"{name}.pdf"
    for name in ["moon-illumination", "cosmics", "cosmics-above", "run-pointings"]
]


rule dl1:
    input:
        out / "runs-linked.txt",
        run_selection_plots,


localrules:
    runlist,
    select_datasets,
    merge_datachecks,
    run_ids,
    data_check,

#+end_src


There is one manual step required before this point:
The runlist has to be downloaded from the lst1 website, which is password-protected.
As I want to have this public, I cannot put the credentials here.
It is just a simple `curl` command, so not a big deal.

*Note:* The analysis will not know of new runs until you redownload the runlist.
That should not matter most of the times, but keep it in mind!

#+begin_src snakemake

rule runlist:
    output:
        out / "runlist.html",
    shell:
        """
        echo 'Provide the file {output}. The command is:'
        echo 'curl --user <username>:<password> https://lst1.iac.es/datacheck/lstosa/LST_source_catalog.html -o {output}'
        echo 'You might need to create the output directory first.'
        """
#+end_src

** Select relevant runs based on runlist

First of all, we need to select runs observing our source(s) of interest.
This is done purely based on the `runlist.html` without any notion of data quality.
There is a column `Source Name` in there, which should match the source.
A natural expansion here would be to select multiple source names at once if thats useful
for the further analysis steps (-> Background model?)

#+begin_src snakemake

rule select_datasets:
    output:
        out / "runlist.csv",
    input:
        data=out / "runlist.html",
        config=config,
        script=scripts / "select-data.py",
    conda:
        env
    log:
        out=out / "select_datasets.log",
        err=out / "select_datasets.err",
    shell:
        "python {input.script} {input.data} {output} -c {input.config}"

#+end_src

** Data quality checks

The next step is discarding runs, that, for one reason or another, do not
qualify for further analysis.
Luckily, [[https://github.com/cta-observatory/lstosa][LSTOSA]] produces datacheck-files, that we can use here.
A datacheck-file contains runwise statistics of some quantities commonly used to gauge
data quality, for example the rate of cosmic events, which directly translates to
some form of efficiency as the rate of hadrons arriving at earth is considered to be constant.

For ease of use, we merge the runwise datachecks first and make the selection on that
merged object.

#+begin_src snakemake

rule merge_datachecks:
    output:
        output=out / "dl1-datachecks-merged.h5",
    input:
        data=out / "runlist.csv",
        script=scripts / "merge-datachecks.py",
    conda:
        env
    log:
        out / "merge_datacheck.log",
    shell:
        "python {input.script} {input.data} {output.output} --log-file {log}"

#+end_src

That selection is based on one of the config files, where e.g. thresholds for the cosmics rate are set.
As a result, the rule produces:
- A list of selected runs
- Cuts and masked datacheck for plots

#+name: data_check
#+begin_src snakemake

rule data_check:
    output:
        runlist=out / "runlist-checked.csv",
        datachecks=out / "dl1-datachecks-masked.h5",
        config=out / "dl1-selection-cuts-config.json",
    input:
        runlist=out / "runlist.csv",
        datachecks=out / "dl1-datachecks-merged.h5",
        config=config,
        script=scripts / "data-check.py",
    conda:
        env
    log:
        out / "datacheck.log",
    shell:
        "python \
            {input.script} \
            {input.runlist} \
            {input.datachecks} \
            --config {input.config} \
            --output-runlist {output.runlist} \
            --output-datachecks {output.datachecks} \
            --output-config {output.config} \
            --log-file {log}"

#+end_src

** Define runs to be used for the analysis

With the list of runs from [[nameref:data_check]], we can go ahead and
link the runs from the global data-directory to our build-directory.
This simplifies rules massively.
Before doing that, we first convert the runlist.
That is pretty arbitrary and could also be done in a single step.
I do not know exactly why we did it this way, but it works, right?


#+begin_src snakemake

checkpoint run_ids:
    output:
        out / "runs.json",
    input:
        data=out / "runlist-checked.csv",
        config=config,
        script=scripts / "create-night-run-list.py",
    conda:
        env
    log:
        out / "check_runlist.log",
    shell:
        "python \
        {input.script} \
        {input.data} \
        {output} \
        -c {input.config} \
        --log-file {log}"

#+end_src

Now we arrive at the first checkpoint!
It is important to have this as a checkpoint, because a priori
you do not know which runs you select for the analysis.
It is known a few steps before this to be exact, but since this is the last
(run-)linking related rule, I decided to make this the checkpoint.
From now on out, all run ids are known just by looking into the `<build_dir>/dl1`
folder.

#+name: link_runs_checkpoint
#+begin_src snakemake

checkpoint link_runs:
    output:
        out / "runs-linked.txt",
    input:
        runs=out / "runs.json",
        datacheck=out / "dl1-datachecks-masked.h5",
        script=scripts / "link-runs.py",
    params:
        dl1=dl1_link_location,
    conda:
        env
    log:
        out / "link_runs.log"
    shell:
        "python \
        {input.script} \
        --runs {input.runs} \
        --dl1-link-dir {params.dl1} \
        --log-file {log} \
        --output-path {output}"

#+end_src

** Plots

Plotting the data-selection part is very easy.
Since multiple plots can be constructed from the output of the [[nameref:data_check][datacheck-rule]],
there is just one [[nameref:plot_data_selection][rule]] to handle these and the script-name is constructed from the wildcard
/name/, which is the name of the output plot.
It could also be multiple rules as not all of them need all of the input files,
but this is how we constructed it a while back for the 1D-analysis.

#+name: plot_data_selection
#+begin_src snakemake

rule plot_data_selection:
    output:
        plots / "{name}.pdf",
    input:
        data=out / "dl1-datachecks-masked.h5",
        config=out / "dl1-selection-cuts-config.json",
        script=scripts / "plot-{name}.py",
    conda:
        env
    log:
        plots / "{name}.log",
    shell:
        "python \
        {input.script} \
        {input.data} \
        -c {input.config} \
        -o {output} \
        --log-file {log} "

#+end_src

For the run pointings, a new file containing just these is constructed.
This is actually not a big step and could be done in the plot script aswell,
but I like having the csv file with the pointing directions easily accesible.

TODO This is not working!

#+begin_src snakemake

rule gather_run_pointings:
    output:
        out / "run-pointings.csv",
    input:
        runs=out / "runs.json",
        datacheck=out / "dl1-datachecks-masked.h5",
        script=scripts / "gather-run-pointings.py",
    conda:
        env
    log:
        out / "run_pointings.log",
    shell:
        "python {input.script} \
        --runs {input.runs} \
        --runsummary {input.datacheck} \
        --output {output} \
        --log-file {log} "


rule plot_run_pointings:
    output:
        plots / "run-pointings.pdf",
    input:
        pointings=out / "run-pointings.csv",
        script=scripts / "plot-run-pointings.py",
    conda:
        env
    log:
        plots / "run_pointings.log",
    shell:
        "python {input.script} \
        --input {input.pointings} \
        --output {output} \
        --log-file {log} "
#+end_src

* MC
:PROPERTIES:
 :header-args:  :tangle ./workflow/rules/mc.smk :mkdirp yes
:END:
** TODO things
Also plots are missing. There should at least be some rf performance plots.

** Variables

#+begin_src snakemake
env = ENVS["lstchain"]
link_env = ENVS["data_selection"]
plot_env = ENVS["gammapy"]
scripts = Path(SCRIPTS["mc"])
mc = Path(OUTDIRS["mc"])
models = mc / "models"

# Need some extra dirs
mc_nodes = Path(OUTDIRS["mc_nodes"])
dl1 = Path(OUTDIRS["dl1"])
models = Path(OUTDIRS["models"])
config = lstchain_config

plots = mc / "plots"

# TODO Configurable
train_size = 0.4



rule mc:
    input:
        link=mc / "mc-linked.txt",
        models=models_to_train,


localrules:
    link_mc,

#+end_src

** Link nodes
#+begin_src snakemake

checkpoint link_mc:
    output:
        dummy=mc / "mc-linked.txt",
        config=config,
    input:
        script=scripts / "link-mc.py",
    params:
        production=PRODUCTION,
        declination=DECLINATION,
        mc_nodes=mc_nodes,
    conda:
        link_env
    log:
        mc / "link_mc.log"
    shell:
        "python \
        {input.script} \
        --prod {params.production} \
        --dec {params.declination} \
        --mc-nodes-link-dir {params.mc_nodes} \
        --model-config-link-path {output.config} \
        --log-file {log} \
        --verbose \
        --output-path {output.dummy}"

#+end_src

** Create train and test files per node

First of all, the individual runs of a single allsky node need to be merged.
After this step there will 2 (train+test) diffuse gamma files per node.

#+begin_src snakemake

rule merge_gamma_mc_per_node:
    output:
        train=mc / "GammaDiffuse/dl1/{node}_train.dl1.h5",
        test=mc / "GammaDiffuse/dl1/{node}_test.dl1.h5",
    input:
        dummy=mc / "mc-linked.txt",
        script=scripts/"merge_mc_nodes.py",
    params:
        train_size=train_size,
        directory=lambda wildcards: mc_nodes / f"GammaDiffuse/{wildcards.node}",
    conda:
        env
    log:
        mc / "GammaDiffuse/dl1/merge_gamma_mc_{node}.log",
    shell:
        "python {input.script} \
        --input-dir {params.directory} \
        --train-size {params.train_size} \
        --output-train {output.train} \
        --output-test {output.test} \
        --log-file {log}"

rule merge_proton_mc_per_node:
    output:
        train=mc / "Protons/dl1/{node}_train.dl1.h5",
    input:
        dummy=mc / "mc-linked.txt",
        script=scripts/"merge_mc_nodes.py",
    params:
        train_size=1.0,
        directory=lambda wildcards: mc_nodes / f"Protons/{wildcards.node}",
    conda:
        env
    log:
        mc / "Protons/dl1/merge_proton_mc_{node}.log",
    shell:
        "python {input.script} \
        --input-dir {params.directory} \
        --train-size {params.train_size} \
        --output-train {output.train} \
        --log-file {log}"

#+end_src

** Train models

There is only one set of models for the whole trajectory and not one for each node in order
to make better use of the training statistic.

#+begin_src snakemake

rule merge_train_or_test_of_all_nodes:
    output:
        mc / "{particle}/{particle}_{train_or_test}.dl1.h5",
    input:
        nodes=MC_NODES_DL1,
        script=scripts/"merge_mc_nodes.py",
    params:
        directory=lambda wildcards: mc / f"{wildcards.particle}/dl1",
        pattern=lambda wildcards: f"*_{wildcards.train_or_test}.dl1.h5",
        out_type=lambda wildcards: f"output-{wildcards.train_or_test}",
    conda:
        env
    log:
        mc / "{particle}/merge_all_{particle}_{train_or_test}.log",
    shell:
        """
        python {input.script} \
        --input-dir {params.directory} \
        --pattern {params.pattern} \
        --{params.out_type} {output} \
        --log-file {log}
        """

#+end_src

For the training it is just the lstchain script.
That requires a lot of resources, because they load all of the data into RAM at once...


#+begin_src snakemake

# TODO Any chance to get logging in here?
rule train_models:
    output:
        models_to_train,
    input:
        gamma=mc / "GammaDiffuse/GammaDiffuse_train.dl1.h5",
        proton=mc / "Protons/Protons_train.dl1.h5",
        config=config,
    resources:
        mem_mb=64000,
        cpus=8,
        partition="long",
        time=1200,
    conda:
        env
    log:
        models / "train_models.log",
    shell:
        """
        lstchain_mc_trainpipe \
        --fg {input.gamma} \
        --fp {input.proton} \
        --config {input.config} \
        --output-dir {models}
        """

#+end_src

* DL2
:PROPERTIES:
 :header-args:  :tangle ./workflow/rules/dl2.smk :mkdirp yes
:END:

** Definition
#+begin_src snakemake
env = ENVS["lstchain"]
irfs = Path(OUTDIRS["irfs"])
irf_config = CONFIGS["irf_tool"]
irf_scripts = Path(SCRIPTS["irfs"])
dl2 = Path(OUTDIRS["dl2"])
dl2_scripts = Path(SCRIPTS["dl2"])
models = Path(OUTDIRS["models"])
config = lstchain_config


rule dl2:
    input:
        irfs=MC_NODES_IRFs,
        runs=DL2_FILES,


#+end_src

** Create DL2

#+name: dl1_to_dl2_rule
#+begin_src snakemake

rule dl1_to_dl2:
    resources:
        mem_mb=64000,
        cpus=4,
    output:
        Path("{somepath}/dl2") / "{base}.dl2.h5",
    input:
        data = Path("{somepath}/dl1") / "{base}.dl1.h5",
        config=config,
        models=models_to_train,
    conda:
        env
    log:
        "{somepath}/dl2/dl1_to_dl2_{base}.log"
    shell:
        """
        lstchain_dl1_to_dl2  \
            --input-file {input.data}  \
            --output-dir $(dirname {output}) \
            --path-models {models}  \
            --config {input.config}
        """

#+end_src

** IRFs
#+begin_src snakemake

# TODO Logging
rule irf:
    output:
        irfs / "irfs_{node}.fits.gz",
    input:
        gammas=mc / "GammaDiffuse/dl1/{node}_test.dl1.h5",
        config=irf_config,
    conda:
        env
    resources:
        mem_mb=8000,
        time=10,
    log:
        irfs / "irfs_{node}.log"
    shell:
        """
        lstchain_create_irf_files \
            -o {output} \
            -g {input.gammas} \
            --config {input.config}
        """
#+end_src

somedir to reuse the rule after lstchain created the effective irf at dl3 stage

#+begin_src snakemake
rule plot_irf:
    output:
        "{somedir}/plots/{irf}_{base}.pdf",
    input:
        data="{somedir}/irfs_{base}.fits.gz",
        script=irf_scripts / "plot_{irf}.py",
        rc=MATPLOTLIBRC,
    conda:
        plot_env
    resources:
        mem_mb=1000,
        time=20,
    log:
        "{somedir}/plots/{irf}_{base}.log"
    shell:
        "MATPLOTLIBRC={input.rc} \
        python {input.script} \
        -i {input.data} \
        -o {output} \
        --log-file {log}"

#+end_src

* DL3
:PROPERTIES:
 :header-args:  :tangle ./workflow/rules/dl3.smk :mkdirp yes
:END:
** definitions
#+begin_src snakemake

env = ENVS["lstchain"]
bkg_env = ENVS["background"]

# Having these as paths makes them easier to use
dl2 = Path(OUTDIRS["dl2"])
dl3 = Path(OUTDIRS["dl3"])
irfs = Path(OUTDIRS["irfs"])
bkg = Path(OUTDIRS["bkg"])
models = Path(OUTDIRS["models"])

irf_config = CONFIGS["irf_tool"]
bkg_config = CONFIGS["bkg_model"]

scripts = Path(SCRIPTS["dl3"])

plots = Path(PLOTSDIRS["dl3"])

# TODO CHeck if this works and then do it for all levels
def dl3_filename(wildcards):
    return f"dl3_LST-1.Run{wildcards.run_id}.fits.fits"


rule dl3:
    output:
        dl3 / "dl3_LST-1.Run{run_id}.fits.gz", # TODO Can i change that naming or does the create_hdu_index script fail then?
    input:
        data=dl2 / "LST-1.Run{run_id}.dl2.h5",
        irf=irfs / "irf_Run{run_id}.fits.gz",
        config=irf_config,
    conda:
        env
    resources:
        mem_mb=12000,
        time=30,
    log:
        out=lambda wildcards, output: output.with_suffix(".log"),
        err=lambda wildcards, output: output.with_suffix(".err"),
    shell:
        """
        lstchain_create_dl3_file  \
            --input-dl2 {input.data}  \
            --output-dl3-path $(dirname $(realpath {output}))  \
            --input-irf {input.irf}  \
            --config {input.config} \
            --gzip \
            --overwrite \
        """


# Using my fork here currently


# TODO Write my own script.
# no clear way to swap between runwise and stacked in my workflow :/
# maybe define a function, that returns the corresponding bkg name to a run based on
# a variable. I would need to parse that from the bkgmodel config and
# also get it into the link bkg script ...
# build_dir / "background/stacked_bkg_map.fits"
# dont ask... result of my hacks, should be solved later upstream
rule calc_background:
    output:
        expand(
            dl3 / "dl3_LST-1.Run{run_id}.fits.fits",
            run_id=RUN_IDS,
        ),
    input:
        runs=expand(
            dl3 / "dl3_LST-1.Run{run_id}.fits.gz",
            run_id=RUN_IDS,
        ),
        config=bkg_config,
    conda:
        bkg_env
    log:
        out=dl3 / "calc_bkg.log",
        err=dl3 / "calc_bkg.err",
    shell:
        """
        bkgmodel --config {input.config}
        """


# Use lstchain env here to ensure we can load it
# run id is stacked only right now, but this way it can be expanded
# data=build_dir / "background/{run_id}_bkg_map.fits", # stacked
rule plot_background:
    output:
        plots / "background/{run_id}.pdf",
    input:
        data=background / "dl3_LST-1.Run{run_id}.fits.fits",  #runwise
        rc=os.environ.get("MATPLOTLIBRC", config_dir / "matplotlibrc"),
        script=scripts / "plot_bkg.py",
    conda:
        env
    log:
        out=lambda wildcards, output: output.with_suffix(".log"),
        err=lambda wildcards, output: output.with_suffix(".err"),
    shell:
        "MATPLOTLIBRC={input.rc} python {input.script} -i {input.data} -o {output}"


# bkg = build_dir / "background/stacked_bkg_map.fits"
# dont ask... result of my hacks, should be solved later upstream
rule dl3_hdu_index:
    output:
        dl3 / "hdu-index.fits.gz",
    input:
        runs=expand(
            dl3 / "dl3_LST-1.Run{run_id}.fits.gz",
            run_id=RUN_IDS,
        ),
        bkg=expand(
            dl3 / "dl3_LST-1.Run{run_id}.fits.fits",
            run_id=RUN_IDS,
        ),
    params:
        bkg_script=scripts / "link_bkg.py",
        bkg_dir=lambda w, input: os.path.relpath(
            Path(input.bkg[0]).parent, Path(input.runs[0]).parent
        ),
        bkg_files=lambda w, input: [Path(x).name for x in input.bkg],
    conda:
        env
    log:
        out=lambda wildcards, output: output.with_suffix(".log"),
        err=lambda wildcards, output: output.with_suffix(".err"),
    resources:
        time=15,
    shell:
        """
        lstchain_create_dl3_index_files  \
            --input-dl3-dir {build_dir}/dl3  \
            --output-index-path {build_dir}/dl3  \
            --file-pattern 'dl3_*.fits.gz'  \
            --overwrite

        python {params.bkg_script} \
        --hdu-index-path {output} \
        --bkg-dir {params.bkg_dir} \
        --bkg-file {params.bkg_files}
        """


# Plots using dl3 files
rule observation_plots:
    input:
        build_dir / "dl3/hdu-index.fits.gz",
        config=config_dir / "{analysis}/analysis.yaml",
        script="scripts/events.py",
    output:
        build_dir / "plots/{analysis}/observation_plots.pdf",
    resources:
        mem_mb=64000,
    conda:
        gammapy_env
    shell:
        """
        python {input.script} \
            -c {input.config} \
            -o {output} \
        """


rule calc_theta2_per_obs:
    output:
        build_dir / "dl3/theta2/{run_id}.fits.gz",
    input:
        data=build_dir / "dl3/dl3_LST-1.Run{run_id}.fits.gz",
        script="scripts/calc_theta2_per_obs.py",
        config=data_selection_config_path,
        index=build_dir / "dl3/hdu-index.fits.gz",
    wildcard_constraints:
        run_id="\d+",  # dont match on "stacked".
    resources:
        mem_mb=16000,
    conda:
        gammapy_env
    log:
        build_dir / "logs/dl3/theta2/{run_id}.log",
    shell:
        "python {input.script} -i {build_dir}/dl3 -o {output} --obs-id {wildcards.run_id} --config {input.config} --log-file {log}"


rule stack_theta2:
    output:
        build_dir / "dl3/theta2/stacked.fits.gz",
    input:
        runs=expand(
            build_dir / "dl3/theta2/{run_id}.fits.gz",
            run_id=RUN_IDS,
        ),
        script="scripts/stack_theta2.py",
    conda:
        gammapy_env
    log:
        build_dir / "logs/dl3/theta2_stacked.log",
    shell:
        "python {input.script} -o {output} --input-files {input.runs} --log-file {log}"


rule plot_theta:
    output:
        build_dir / "plots/theta2/{runid}.pdf",
    input:
        data=build_dir / "dl3/theta2/{runid}.fits.gz",
        script="scripts/plot_theta2.py",
        rc=os.environ.get("MATPLOTLIBRC", config_dir / "matplotlibrc"),
    conda:
        gammapy_env
    shell:
        "MATPLOTLIBRC={input.rc} python {input.script} -i {input.data} -o {output}"


rule bkg_exclusion:
    input:
        config=config_dir / "{analysis}/analysis.yaml",
        script="scripts/make_exclusion_region.py",
    output:
        build_dir / "{analysis}/exclusion.fits.gz",
    conda:
        gammapy_env
    shell:
        "python {input.script} -c {input.config} -o {output}"


rule calc_skymap_per_obs:
    output:
        build_dir / "dl3/skymap_dl3/{run_id}.fits",
    input:
        data=build_dir / "dl3/dl3_LST-1.Run{run_id}.fits.gz",
        script="scripts/calc_skymap_gammas.py",
        config=irf_config_path,
        index=build_dir / "dl3/hdu-index.fits.gz",
    wildcard_constraints:
        run_id="\d+",  # dont match on "stacked".
    resources:
        # mem_mb=16000,
        time=5,
    conda:
        gammapy_env
    shell:
        "python {input.script} -i {build_dir}/dl3 -o {output} --obs-id {wildcards.run_id} --config {input.config}"


rule plot_skymap_dl3:
    output:
        build_dir / "plots/skymap_dl3/{runid}.pdf",
    input:
        data=build_dir / "dl3/skymap_dl3/{runid}.fits",
        script="scripts/plot_skymap_dl3.py",
        rc=os.environ.get("MATPLOTLIBRC", config_dir / "matplotlibrc"),
    conda:
        gammapy_env
    resources:
        time=5,
    shell:
        "MATPLOTLIBRC={input.rc} python {input.script} -i {input.data} -o {output}"


rule calc_skymap:
    resources:
        mem_mb="64G",
        time=10,
    conda:
        lstchain_env
    output:
        build_dir / "dl3/skymap/{run_id}.fits",
    input:
        data=build_dir / "dl2/dl2_LST-1.Run{run_id}.h5",  #?????????
        config=irf_config_path,
        script="scripts/calc_skymap.py",
    shell:
        "python {input.script} -i {input.data} -o {output} -c {input.config}"


rule plot_skymap:
    conda:
        lstchain_env
    output:
        build_dir / "plots/skymap/{run_id}.pdf",
    input:
        data=build_dir / "dl3/skymap/{run_id}.fits",
        script="scripts/plot_skymap.py",
        rc=os.environ.get("MATPLOTLIBRC", config_dir / "matplotlibrc"),
    shell:
        "MATPLOTLIBRC={input.rc} python {input.script} -i {input.data} -o {output}"


rule stack_skymaps:
    conda:
        lstchain_env
    output:
        build_dir / "dl3/skymap/stacked.fits",
    input:
        data=expand(
            build_dir / "dl3/skymap/{run_id}.fits",
            run_id=RUN_IDS,
        ),
        script="scripts/stack_skymap.py",
    shell:
        "python {input.script} -i {input.data} -o {output}"


rule stack_skymaps_dl3:
    conda:
        lstchain_env
    output:
        build_dir / "dl3/skymap_dl3/stacked.fits",
    input:
        data=expand(
            build_dir / "dl3/skymap_dl3/{run_id}.fits",
            run_id=RUN_IDS,
        ),
        script="scripts/stack_skymap.py",
    shell:
        "python {input.script} -i {input.data} -o {output}"

#+end_src

** Cuts dl3 (kinda irf)
This wont work as it needs the "final" irf file!
#+begin_src snakemake

rule cuts_dl2_dl3:
    resources:
        mem_mb="64G",
        time=10,
    conda:
        lstchain_env
    output:
        build_dir / "dl3/counts/after_gh_theta_cut_{run_id}.h5",
    input:
        dl2=build_dir / "dl2/dl2_LST-1.Run{run_id}.h5",
        irf=build_dir / "irf/irf_Run{run_id}.fits.gz",
        config=irf_config_path,
        script="scripts/calc_counts_after_cuts.py",
    shell:
        "python {input.script} --input-dl2 {input.dl2} --input-irf {input.irf} -c {input.config} -o {output}"



#+end_src

#+begin_src snakemake


rule stack_cuts_dl2_dl3:
    conda:
        lstchain_env
    output:
        build_dir / "dl3/counts/after_gh_theta_cut_{norm}_stacked.h5",
    input:
        data=expand(
            build_dir / "dl3/counts/after_gh_theta_cut_{run_id}.h5",
            run_id=RUN_IDS,
        ),
        script="scripts/stack_counts_after_cuts.py",
        rc=os.environ.get("MATPLOTLIBRC", config_dir / "matplotlibrc"),
    shell:
        "MATPLOTLIBRC={input.rc} python {input.script} -i {input.data} -o {output} --norm {wildcards.norm}"

#+end_src

#+begin_src snakemake

rule plot_cuts_dl2_dl3:
    conda:
        lstchain_env
    output:
        build_dir / "plots/counts_after_gh_theta_cut_{norm}.pdf",
    input:
        data=build_dir / "dl3/counts/after_gh_theta_cut_{norm}.h5",
        script="scripts/plot_counts_after_cuts.py",
        rc=os.environ.get("MATPLOTLIBRC", config_dir / "matplotlibrc"),
    shell:
        "MATPLOTLIBRC={input.rc} python {input.script} -i {input.data} -o {output}"



#+end_src

* DL4
:PROPERTIES:
 :header-args:  :tangle ./workflow/rules/dl4.smk :mkdirp yes
:END:

#+begin_src snakemake

# Create DL4 datasets, plot sensitivity, significance, ...
rule dataset_3d:
    input:
        data=build_dir / "dl3/hdu-index.fits.gz",
        config=config_dir / "{analysis}/analysis.yaml",
        script="scripts/write_datasets_manual.py",
    output:
        build_dir / "dl4/{analysis}/datasets.fits.gz",
    conda:
        gammapy_env
    shell:
        "python {input.script} -c {input.config} -o {output}"


rule calc_sensitivity:
    input:
        data=build_dir / "dl4/{analysis}/datasets.fits.gz",
        script="scripts/calc_sensitivity.py",
    output:
        build_dir / "dl4/{analysis}/sensitivity.fits.gz",
    conda:
        gammapy_env
    shell:
        """
        python {input.script} \
            --dataset-path {input.data} \
            -o {output}
        """


rule calc_dl4_diagnostics:
    output:
        build_dir / "dl4/{analysis}/dl4_diagnostics.fits.gz",
    input:
        data=build_dir / "dl4/{analysis}/datasets.fits.gz",
        config=config_dir / "{analysis}/analysis.yaml",
        script="scripts/calc_dl4_diagnostics.py",
    resources:
        mem_mb=16000,
    conda:
        gammapy_env
    shell:
        "python {input.script} -c {input.config} -o {output} --dataset-path {input.data}"


rule peek_datasets:
    output:
        build_dir / "plots/{analysis}/dataset_peek.pdf",
    input:
        data=build_dir / "dl4/{analysis}/datasets.fits.gz",
        script="scripts/plot_dataset_peek.py",
        config=config_dir / "{analysis}/analysis.yaml",
        rc=os.environ.get("MATPLOTLIBRC", config_dir / "matplotlibrc"),
    conda:
        gammapy_env
    shell:
        "MATPLOTLIBRC={input.rc} python {input.script} -c {input.config} -o {output} --dataset-path {input.data}"


rule plot_dl4:
    output:
        build_dir / "plots/{analysis}/{name}.pdf",
    input:
        data=build_dir / "dl4/{analysis}/{name}.fits.gz",
        script="scripts/plot_{name}.py",
        rc=os.environ.get("MATPLOTLIBRC", config_dir / "matplotlibrc"),
    conda:
        gammapy_env
    shell:
        "MATPLOTLIBRC={input.rc} python {input.script} -i {input.data} -o {output}"
#+end_src

* DL5
:PROPERTIES:
 :header-args:  :tangle ./workflow/rules/dl5.smk :mkdirp yes
:END:

#+begin_src snakemake

rule calc_significance_map:
    output:
        build_dir / "dl4/{analysis}/significance_map.fits.gz",
    input:
        data=build_dir / "dl4/{analysis}/datasets.fits.gz",
        script="scripts/calc_significance_map.py",
        config=config_dir / "{analysis}/analysis.yaml",
    conda:
        gammapy_env
    shell:
        "python {input.script} -c {input.config} -o {output} --dataset-path {input.data}"


rule plot_significance_map:
    output:
        build_dir / "plots/{analysis}/significance_map.pdf",
    input:
        lima_map=build_dir / "dl4/{analysis}/significance_map.fits.gz",
        exclusion_mask=build_dir / "{analysis}/exclusion.fits.gz",
        script="scripts/plot_significance_map.py",
        rc=os.environ.get("MATPLOTLIBRC", config_dir / "matplotlibrc"),
    conda:
        gammapy_env
    shell:
        "MATPLOTLIBRC={input.rc} python {input.script} --lima-maps-input {input.lima_map} --exclusion-map-input {input.exclusion_mask} -o {output}"


# Fit flux etc.
rule calc_flux_points:
    input:
        data=build_dir / "dl4/{analysis}/datasets.fits.gz",
        config=config_dir / "{analysis}/analysis.yaml",
        model=build_dir / "dl4/{analysis}/model-best-fit.yaml",
        script="scripts/calc_flux_points.py",
    output:
        build_dir / "dl4/{analysis}/flux_points.fits.gz",
    conda:
        gammapy_env
    shell:
        """
        python {input.script} \
            -c {input.config} \
            --dataset-path {input.data} \
            --best-model-path {input.model} \
            -o {output}
        """


rule plot_flux_points:
    input:
        data=build_dir / "dl4/{analysis}/flux_points.fits.gz",
        model=build_dir / "dl4/{analysis}/model-best-fit.yaml",
        script="scripts/plot_flux_points.py",
    output:
        build_dir / "plots/{analysis}/flux_points.pdf",
    conda:
        gammapy_env
    shell:
        """
        python {input.script} \
            -i {input.data} \
            --best-model-path {input.model} \
            -o {output}
        """


rule calc_light_curve:
    input:
        model=build_dir / "dl4/{analysis}/model-best-fit.yaml",
        config=config_dir / "{analysis}/analysis.yaml",
        dataset=build_dir / "dl4/{analysis}/datasets.fits.gz",
        script="scripts/calc_light_curve.py",
    output:
        build_dir / "dl4/{analysis}/light_curve.fits.gz",
    conda:
        gammapy_env
    shell:
        """
        python {input.script} \
            -c {input.config} \
            --dataset-path {input.dataset} \
            --best-model-path {input.model} \
            -o {output} \
        """


rule model_best_fit:
    input:
        config=config_dir / "{analysis}/analysis.yaml",
        dataset=build_dir / "dl4/{analysis}/datasets.fits.gz",
        model=config_dir / "{analysis}/models.yaml",
        script="scripts/fit-model.py",
    output:
        build_dir / "dl4/{analysis}/model-best-fit.yaml",
    conda:
        gammapy_env
    shell:
        """
        python {input.script} \
            -c {input.config} \
            --dataset-path {input.dataset} \
            --model-config {input.model} \
            -o {output} \
        """
#+end_src

* DM
:PROPERTIES:
 :header-args:  :tangle ./workflow/rules/dm.smk :mkdirp yes
:END:

#+begin_src snakemake
#TODO
#+end_src
